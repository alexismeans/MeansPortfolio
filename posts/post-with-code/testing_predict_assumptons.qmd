---
title: "Checking Bio Predict Assumptions"
author: "Alexis Means"
date: "2025-10-2"
description: This document contains assumption tests and plots to analyze my linear regression equations for biomass predictions. 
format:
  html:
    toc: true
    toc-location: left
    toc-depth: 3
editor: visual
code-fold: true
---

```{r}
spp_model <- readRDS("C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/regression_equations/24and25-Biomass-Regression-Species-Top-Model-List.rds")

fam_model <- readRDS("C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/regression_equations/24and25-Biomass-Regression-Family-Top-Model-List.rds")

genus_model <- readRDS("C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/regression_equations/24and25-Biomass-Regression-Genus-Top-Model-List.rds")

fg_model <- readRDS("C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/regression_equations/24and25-Biomass-Regression-Functional-Group-Top-Model-List.rds")

all_models <- list(
  spp   = spp_model,
  fam   = fam_model,
  genus = genus_model,
  fg    = fg_model
)

```

## Predicted Biomass Values

Here are my current predicted biomass values based on the linear equations at the bottom of the page. I didn’t include the code I used to generate them in this document, but I can send it over if you’d like to take a look. Right now, all the biomass values are in grams rather than kilograms. I’m not sure if that affects the predictions, but my plan was to scale them up to kilograms before rerunning the FRESH model.

```{r}
#| warning: FALSE
library(readr)
library(readxl)
library(DT)

df <- "C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/24and25predictions.xlsx"

predictions <- read_excel(df, "Sheet1")

datatable(predictions, options = list(pageLength = 10))
```

## R2, AIC and Coefficients

A lot of the r-squared values look much better. The FG results still do not look great.

```{r}
spp <- "C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/processed.data/spp_models_diagnostics.xlsx"

spp_predictions <- read_excel(spp, "Sheet1")

datatable(spp_predictions, options = list(pageLength = 10))
```

```{r}
genus <- "C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/processed.data/genus_models_diagnostics.xlsx"

genus_predictions <- read_excel(genus, "Sheet1")

datatable(genus_predictions, options = list(pageLength = 10))
```

```{r}
fam <- "C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/processed.data/family_models_diagnostics.xlsx"

fam_predictions <- read_excel(fam, "Sheet1")

datatable(fam_predictions, options = list(pageLength = 10))
```

```{r}
fg <- "C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/processed.data/fg_models_diagnostics.xlsx"

fg_predictions <- read_excel(fg, "Sheet1")

datatable(fg_predictions, options = list(pageLength = 10))
```

## Assumption Test P-values

I ran assumption tests on only the models that did not have a single model with an r-squared value higher than 0.4 (this was the value that Katey used, I can adjust it if needed)

-   Breusch–Pagan test (bptest) for homoscedasticity

-   Shapiro–Wilk test for normality of residuals

-   Durbin–Watson test (dwtest) for autocorrelation

-   Cook’s distance for influential observations

I’m not sure if there are other tests that would be better to run instead. I could also use some guidance on which p-values should be considered “red flags” that suggest I need to use a different equation. I rounded everything to four decimal places to make the results easier to read, so any zeros you see are just very small values above zero

```{r}
#| warning: FALSE
library(tidyverse)
library(lmtest)
library(purrr)
library(DT)

check_lm_assumptions <- function(model) {
  if (!inherits(model, "lm")) return(NULL)
  
  res <- residuals(model)
  
  list(
    r_squared = tryCatch(round(summary(model)$r.squared, 4), error = function(e) NA),
    shapiro_p = tryCatch(round(shapiro.test(res)$p.value, 4), error = function(e) NA),
    bp_test   = tryCatch(round(lmtest::bptest(model)$p.value, 4), error = function(e) NA),
    dw_test   = tryCatch(round(lmtest::dwtest(model)$p.value, 4), error = function(e) NA),
    cook_max  = tryCatch(round(max(cooks.distance(model), na.rm = TRUE), 4), error = function(e) NA)
  )
}

diagnostics <- imap_dfr(all_models, function(model_list, group_name) {
  map_dfr(model_list, check_lm_assumptions, .id = "model_name") %>%
    mutate(group = group_name, .before = 1)
})

#--------------------------------------------------------------
# 3️⃣ Extract model base name (everything before trailing number)
#     Example: "BRTE_GREEN1" → "BRTE_GREEN"
#--------------------------------------------------------------
diagnostics <- diagnostics %>%
  mutate(model_base = str_trim(str_extract(model_name, ".*(?=\\d+$)")))

#--------------------------------------------------------------
# 4️⃣ Identify and remove model families with any R² > 0.4
#--------------------------------------------------------------
high_r2_bases <- diagnostics %>%
  group_by(model_base) %>%
  summarize(any_high_r2 = any(r_squared > 0.4, na.rm = TRUE)) %>%
  filter(any_high_r2) %>%
  pull(model_base)

diagnostics_low_r2 <- diagnostics %>%
  filter(!model_base %in% high_r2_bases)


datatable(diagnostics_low_r2, options = list(pageLength = 10))
```

## Diagnostic Plots and Equations

Here are the diagnostic plots for each of the linear models: Breusch–Pagan test (top left), Shapiro–Wilk test (top right), Durbin–Watson test (bottom left), and Cook’s distance (bottom right). Below the plots, I’ve also included each of the linear equations I’m using to predict biomass.

```{r}
#| warning: FALSE
#| eval: FALSE
library(ggfortify)
library(gridExtra)

plot_diagnostics <- function(model, model_name, group_name) {
  
  p1 <- autoplot(model, which = 1)[[1]] + 
    ggtitle(paste("Residuals vs Fitted for", model_name, "in", group_name))
  
  p2 <- autoplot(model, which = 2)[[1]] + 
    ggtitle(paste("Normal Q-Q for", model_name, "in", group_name))
  
  p3 <- autoplot(model, which = 3)[[1]] + 
    ggtitle(paste("Scale-Location for", model_name, "in", group_name))
  
  p4 <- autoplot(model, which = 5)[[1]] + 
    ggtitle(paste("Residuals vs Leverage for", model_name, "in", group_name))
  
  gridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2)
}

imap(all_models, function(model_list, group_name) {
  
  # Filter only models from this group that passed the R² criteria
  keep_models <- diagnostics_low_r2 %>%
    filter(group == group_name) %>%
    pull(model_name)
  
  # Subset model list to those models
  model_list_filtered <- model_list[names(model_list) %in% keep_models]
  
  # Plot only the retained models
  walk2(model_list_filtered, names(model_list_filtered), ~ {
    plot_diagnostics(.x, .y, group_name)
  })
})
```

## What's next

I am going to rerun the prediction equations with an interaction term between JD and cover like we discussed in lab meeting and see how that improves any of the models.
