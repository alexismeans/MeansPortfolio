---
title: "Checking Bio Predict Assumptions"
author: "Alexis Means"
date: "2025-10-2"
description: This document contains assumption tests and plots to analyze my linear regression equations for biomass predictions. 
format:
  html:
    toc: true
    toc-location: left
    toc-depth: 3
editor: visual
code-fold: true
---

```{r}
spp_model <- readRDS("C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/regression_equations/24and25-Biomass-Regression-Species-Top-Model-List.rds")

fam_model <- readRDS("C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/regression_equations/24and25-Biomass-Regression-Family-Top-Model-List.rds")

genus_model <- readRDS("C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/regression_equations/24and25-Biomass-Regression-Genus-Top-Model-List.rds")

fg_model <- readRDS("C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/regression_equations/24and25-Biomass-Regression-Functional-Group-Top-Model-List.rds")

all_models <- list(
  spp   = spp_model,
  fam   = fam_model,
  genus = genus_model,
  fg    = fg_model
)

```

## Predicted Biomass Values

Here are my current predicted biomass values based on the linear equations at the bottom of the page. I didn’t include the code I used to generate them in this document, but I can send it over if you’d like to take a look. Right now, all the biomass values are in grams rather than kilograms. I’m not sure if that affects the predictions, but my plan was to scale them up to kilograms before rerunning the FRESH model.

```{r}
#| warning: FALSE
library(readr)
library(readxl)
library(DT)

df <- "C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/24and25predictions.xlsx"

predictions <- read_excel(df, "Sheet1")

datatable(predictions, options = list(pageLength = 10))
```

## R2, AIC and Coefficients

A lot of the r-squared values look much better. The FG results still do not look great.

```{r}
spp <- "C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/processed.data/spp_models_diagnostics.xlsx"

spp_predictions <- read_excel(spp, "Sheet1")

datatable(spp_predictions, options = list(pageLength = 10))
```

```{r}
genus <- "C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/processed.data/genus_models_diagnostics.xlsx"

genus_predictions <- read_excel(genus, "Sheet1")

datatable(genus_predictions, options = list(pageLength = 10))
```

```{r}
fam <- "C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/processed.data/family_models_diagnostics.xlsx"

fam_predictions <- read_excel(fam, "Sheet1")

datatable(fam_predictions, options = list(pageLength = 10))
```

```{r}
fg <- "C:/Users/Alexis Means/Documents/Project/Nutrition Sampling/R code/biomass/processed.data/fg_models_diagnostics.xlsx"

fg_predictions <- read_excel(fg, "Sheet1")

datatable(fg_predictions, options = list(pageLength = 10))
```

## Assumption Test P-values

I ran assumption tests on only the models that did not have a single model with an r-squared value higher than 0.4 (this was the value that Katey used, I can adjust it if needed)

-   Breusch–Pagan test (bptest) for homoscedasticity

-   Shapiro–Wilk test for normality of residuals

-   Durbin–Watson test (dwtest) for autocorrelation

-   Cook’s distance for influential observations

I’m not sure if there are other tests that would be better to run instead. I could also use some guidance on which p-values should be considered “red flags” that suggest I need to use a different equation. I rounded everything to four decimal places to make the results easier to read, so any zeros you see are just very small values above zero

```{r}
#| warning: FALSE
library(tidyverse)
library(lmtest)
library(purrr)
library(DT)

check_lm_assumptions <- function(model) {
  if (!inherits(model, "lm")) return(NULL)
  
  res <- residuals(model)
  
  list(
    r_squared = tryCatch(round(summary(model)$r.squared, 4), error = function(e) NA),
    shapiro_p = tryCatch(round(shapiro.test(res)$p.value, 4), error = function(e) NA),
    bp_test   = tryCatch(round(lmtest::bptest(model)$p.value, 4), error = function(e) NA),
    dw_test   = tryCatch(round(lmtest::dwtest(model)$p.value, 4), error = function(e) NA),
    cook_max  = tryCatch(round(max(cooks.distance(model), na.rm = TRUE), 4), error = function(e) NA)
  )
}

diagnostics <- imap_dfr(all_models, function(model_list, group_name) {
  map_dfr(model_list, check_lm_assumptions, .id = "model_name") %>%
    mutate(group = group_name, .before = 1)
})

#--------------------------------------------------------------
# 3️⃣ Extract model base name (everything before trailing number)
#     Example: "BRTE_GREEN1" → "BRTE_GREEN"
#--------------------------------------------------------------
diagnostics <- diagnostics %>%
  mutate(model_base = str_trim(str_extract(model_name, ".*(?=\\d+$)")))

#--------------------------------------------------------------
# 4️⃣ Identify and remove model families with any R² > 0.4
#--------------------------------------------------------------
high_r2_bases <- diagnostics %>%
  group_by(model_base) %>%
  summarize(any_high_r2 = any(r_squared > 0.4, na.rm = TRUE)) %>%
  filter(any_high_r2) %>%
  pull(model_base)

diagnostics_low_r2 <- diagnostics %>%
  filter(!model_base %in% high_r2_bases)


datatable(diagnostics_low_r2, options = list(pageLength = 10))
```

## Diagnostic Plots and Equations

```{r}
#| message: false
#| warning: false
#| fig.height: 4
#| fig.width: 10
#| fig.align: center


library(patchwork)  # <-- important for combining plots

# Create color grouping
predictions <- predictions %>%
  mutate(source_group = ifelse(pred_source == "Observed", "Observed", "Predicted"))

# Get unique species list
species_list <- unique(predictions$Spp)

# Loop through species and print each combined plot
for (sp in species_list) {
  dat <- filter(predictions, Spp == sp)
  
  # Plot 1: x = ID
  p1 <- ggplot(dat, aes(x = factor(ID), y = pred_DryWeight, color = source_group)) +
    geom_point(size = 2, alpha = 0.8) +
    scale_color_manual(values = c("Observed" = "steelblue", "Predicted" = "orange")) +
    labs(
      title = "By Observation ID",
      x = "Observation ID",
      y = "Predicted Dry Weight (g)",
      color = "Data Source"
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(face = "bold", size = 11),
      legend.position = "top",
      legend.direction = "vertical",
      axis.text.x = element_text(angle = 45, hjust = 1)
    )
  
  # Plot 2: x = Percent
  p2 <- ggplot(dat, aes(x = Percent, y = pred_DryWeight, color = source_group)) +
    geom_point(size = 2, alpha = 0.8) +
    scale_color_manual(values = c("Observed" = "steelblue", "Predicted" = "orange")) +
    labs(
      title = "By Percent Cover",
      x = "Percent",
      y = "Predicted Dry Weight (g)",
      color = "Data Source"
    ) +
    theme_bw() +
    theme(
      plot.title = element_text(face = "bold", size = 11),
      legend.position = "top",
      legend.direction = "vertical"
    )
  
  # Combine side-by-side and share legend vertically above both plots
  combined <- (p1 + p2) +
    plot_layout(guides = "collect") &
    theme(legend.position = "top", legend.direction = "vertical")
  
  # Add species name as a single annotation above everything
  final_plot <- combined +
    plot_annotation(
      title = paste("Predicted Dry Weight for", sp),
      theme = theme(
        plot.title = element_text(size = 14, face = "bold", hjust = 0.5)
      )
    )
  
  print(final_plot)
  cat("\n\n")
}
```

## What's next

I am going to rerun the prediction equations with an interaction term between JD and cover like we discussed in lab meeting and see how that improves any of the models.
